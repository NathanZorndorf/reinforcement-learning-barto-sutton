{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "\n",
    "# Copyright (c) 2023 Nathan Zorndorf\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment \n",
    "# perform actions (action set based on current state), get next_state and reward \n",
    "# setup action array - 2D array with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GamblersProblem:\n",
    "    def __init__(self, p_h) -> None:\n",
    "        self.p_h = p_h\n",
    "        self.state = 50 \n",
    "        self.state_space = list(range(0,100))\n",
    "        return\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 50 \n",
    "        return\n",
    "    \n",
    "    def action_space(self):\n",
    "        return list(range(0, min(self.state, 100-self.state))) # caps action space to [0-50] and thus resulting state space to [0,100]\n",
    "    \n",
    "    def random_choice(self):\n",
    "        choices = [1, -1]\n",
    "        probabilities = [self.p_h, 1 - self.p_h]\n",
    "        result = np.random.choice(choices, p=probabilities)\n",
    "        return result\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        \"\"\"\n",
    "        Run one timestep of the environment's dynamics.\n",
    "\n",
    "        Args:\n",
    "            state (int): An int storing the number of dollars available to bet\n",
    "            action (int): The number of dollars to bet \n",
    "\n",
    "        Returns:\n",
    "            (Tuple[int, float]): The new state of the system, and its correspondent reward\n",
    "        \"\"\"\n",
    "        # if state == terminal state, return associated reward \n",
    "        if state == 0:\n",
    "            reward = 0\n",
    "            next_state = state \n",
    "        elif state == 100:\n",
    "            reward = 1  \n",
    "            next_state = state \n",
    "        else:\n",
    "            reward = 0 \n",
    "            next_state = action * self.random_choice() # sample from bernoulli distribution with probability p_h \n",
    "\n",
    "        return next_state, reward \n",
    "    \n",
    "    def bellman_expectation(self, state, action):\n",
    "        discount = 1\n",
    "        next_state, _ = self.step(state, action)\n",
    "        sum(\n",
    "            self.p_h * (int(state + action >= 100) + discount * self.value_function[state + action]),\n",
    "            (1-self.p_h) * (0 + discount * self.value_function[state - action])\n",
    "\n",
    "        )\n",
    "        return \n",
    "    \n",
    "    def value_iteration(self, epsilon=0.01):\n",
    "        \n",
    "        # initialize value function \n",
    "        self.value_function = np.zeros(len(self.state_space))\n",
    "\n",
    "        delta = 0 \n",
    "        while(delta >= epsilon):   \n",
    "            for state in self.state_space:\n",
    "                old_v = self.value_function[state]\n",
    "                self.value_function[state] = max(\n",
    "                    [self.bellman_expectation(state, action) for action in self.action_space(state)]\n",
    "                )\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GamblersProblem(p_h=0.4)\n",
    "env.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
