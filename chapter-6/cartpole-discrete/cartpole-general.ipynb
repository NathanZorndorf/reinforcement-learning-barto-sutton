{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import imageio\n",
    "import sys \n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, algorithm='SARSA'):\n",
    "        self.algorithm = algorithm\n",
    "        # self.env = gym.make(\"CartPole-v0\",  render_mode=\"rgb_array\")\n",
    "        self.env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "        self.num_episodes = 10000\n",
    "        self.max_steps_per_episode = 500\n",
    "        self.learning_rate = 0.1\n",
    "        self.max_learning_rate = 0.1\n",
    "        self.min_learning_rate = 0.1\n",
    "        self.learning_rate_decay = 0.99\n",
    "        self.discount_rate = 1.0\n",
    "        self.epsilon = 1.0\n",
    "        self.max_epsilon = 1.0\n",
    "        self.min_epsilon = 0.01\n",
    "        self.epsilon_decay_rate = 0.999\n",
    "        self.et_lambda = 0.5 # for eligibility traces\n",
    "        # initialize \n",
    "        self.create_bins()\n",
    "        self.initialize_q_table()\n",
    "\n",
    "    def create_bins(self, bins=10):\n",
    "        cart_pos_bins = np.linspace(-2.4, 2.4, bins+1)\n",
    "        cart_vel_bins = np.linspace(-5.0, 5.0, bins+1)\n",
    "        pole_ang_bins = np.linspace(-0.20, 0.20, bins+1)\n",
    "        pole_tipvel_bins = np.linspace(-2.5, 2.5, bins+1)\n",
    "        self.bins = [cart_pos_bins, cart_vel_bins, pole_ang_bins, pole_tipvel_bins]\n",
    "\n",
    "    def discretize_state(self, observation):\n",
    "        state_signal = 0  # row position in Q-matrix or eligibility trace, equivalent state\n",
    "        bin_state = np.zeros_like(observation)\n",
    "        for i in range(len(observation)):\n",
    "            bin_state[i] = np.digitize(observation[i], self.bins[i])\n",
    "            state_signal += bin_state[i] * ((len(self.bins[i]) + 1) ** i)\n",
    "        return int(state_signal)\n",
    "\n",
    "    def zero_eligibility_traces(self):\n",
    "        self.eligibility_traces = np.zeros_like(self.q_table)\n",
    "\n",
    "    def greedy_action_selection(self, state):\n",
    "        return np.argmax(self.q_table[state, :])\n",
    "    \n",
    "    def eps_action_selection(self, state):\n",
    "        epsilon_threshold = random.uniform(0,1)\n",
    "        if epsilon_threshold > self.epsilon:\n",
    "            # exploitation\n",
    "            return np.argmax(self.q_table[state,:])\n",
    "        else:\n",
    "            # epsilon\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "    def initialize_q_table(self):\n",
    "        num_states = (len(self.bins[0]) + 1) * (len(self.bins[1]) + 1) * (len(self.bins[2]) + 1) * (len(self.bins[3]) + 1)\n",
    "        # self.q_table = np.zeros((num_states, self.env.action_space.n))\n",
    "        self.q_table = np.random.rand(num_states, self.env.action_space.n)\n",
    "\n",
    "        \n",
    "    def decay_epsilon(self, episode):\n",
    "        decay_factor = 1.0 - episode / self.num_episodes\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * decay_factor\n",
    "\n",
    "    def decay_learning_rate(self, episode):\n",
    "        decay_factor = 1.0 - episode / self.num_episodes\n",
    "        self.learning_rate = self.min_learning_rate + (self.max_learning_rate - self.min_learning_rate) * decay_factor\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # initialize arrays to keep track of rewards and epsilon\n",
    "        self.scores = []\n",
    "        self.epsilons = []\n",
    "\n",
    "        # iterate over episodes\n",
    "        for episode in tqdm(range(self.num_episodes)):\n",
    "            \n",
    "            # Adding Eligibility traces\n",
    "            # self.zero_eligibility_traces()\n",
    "\n",
    "            # initialize new episode params\n",
    "            state, _ = self.env.reset()\n",
    "            state = self.discretize_state(state)\n",
    "            done = False\n",
    "            self.rewards = 0\n",
    "\n",
    "            # select new action\n",
    "            if self.algorithm == 'SARSA':\n",
    "                action = self.eps_action_selection(state)\n",
    "            \n",
    "            # iterate over steps in episode\n",
    "            for t in range(self.max_steps_per_episode): \n",
    "                self.env.render()\n",
    "                \n",
    "                # select new action \n",
    "                if self.algorithm == 'Q-learning':\n",
    "                    action = self.eps_action_selection(state)\n",
    "\n",
    "                # Take action \n",
    "                new_state, reward, done, _, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(new_state)\n",
    "\n",
    "                # Experimenting with very bad reward if agent fails\n",
    "                # if done:\n",
    "                    # reward = -1\n",
    "\n",
    "                # select next action \n",
    "                if self.algorithm == 'SARSA':\n",
    "                    new_action = self.eps_action_selection(new_state)      \n",
    "\n",
    "                # self.eligibility_traces[state, action] += 1\n",
    "                if self.algorithm == 'Q-learning':\n",
    "                    self.q_table[state, action] += self.learning_rate * (reward + self.discount_rate * np.max(self.q_table[new_state, :]) - self.q_table[state, action])  # * self.eligibility_traces[state, action]\n",
    "                elif self.algorithm == 'SARSA':\n",
    "                    self.q_table[state, action] += self.learning_rate * (reward + self.discount_rate * self.q_table[new_state, new_action] - self.q_table[state, action]) # * self.eligibility_traces[state, action]\n",
    "                # self.eligibility_traces *= self.discount_rate * self.et_lambda\n",
    "\n",
    "                # Set new state\n",
    "                state = new_state\n",
    "                if self.algorithm == 'SARSA':\n",
    "                    action = new_action\n",
    "\n",
    "                # Add new reward        \n",
    "                self.rewards += reward\n",
    "\n",
    "                # check if done, and if so end the episode \n",
    "                if done or t == self.max_steps_per_episode:\n",
    "                    self.scores.append(self.rewards)\n",
    "                    self.epsilons.append(self.epsilon)\n",
    "                    if (episode + 1) % 10 == 0:\n",
    "                        clear_output(wait=True)\n",
    "                        print(\"episode: {}/{}, time: {}, e: {:.2}\".format(episode, self.num_episodes, t, self.epsilon), flush=True)\n",
    "                    if (episode + 1) % 100 == 0:    \n",
    "                        self.plot_metrics()\n",
    "                        self.create_video()\n",
    "                    break\n",
    "\n",
    "\n",
    "            # Exploration rate decay\n",
    "            self.decay_epsilon(episode)   \n",
    "            self.decay_learning_rate(episode)\n",
    "    \n",
    "        # close environment when finished\n",
    "        self.env.close()\n",
    "\n",
    "    def create_video(self, filename=\"cartpole_dqn_video.mp4\"):\n",
    "        state, _ = self.env.reset()\n",
    "        discretized_state = self.discretize_state(state)\n",
    "        frames = []\n",
    "        for t in range(self.max_steps_per_episode):\n",
    "            frames.append(self.env.render())\n",
    "            action = self.greedy_action_selection(discretized_state)\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            discretized_next_state = self.discretize_state(next_state)\n",
    "            discretized_state = discretized_next_state\n",
    "            if done:\n",
    "                break\n",
    "        imageio.mimsave(filename, frames, fps=30)\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        # Clear the matplotlib figure for the next plot\n",
    "        plt.clf()\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(self.scores)\n",
    "        plt.title('Score per Episode vs Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Score')\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(self.epsilons)\n",
    "        plt.title('Epsilon vs Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Epsilon')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('metrics.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Display the plot\n",
    "        display(plt.gcf())\n",
    "        \n",
    "        # Pause to give Jupyter time to update the display\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Clear the output of the current cell\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6379/10000, time: 102, e: 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6384/10000 [14:59<32:51,  1.83it/s]"
     ]
    }
   ],
   "source": [
    "agent = Agent(algorithm='SARSA')\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
