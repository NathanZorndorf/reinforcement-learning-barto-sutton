{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(agent, env, filename=\"cartpole_dqn_video.mp4\"):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, agent.state_size])\n",
    "    frames = []\n",
    "    for time in range(500):\n",
    "        frames.append(env.render())\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    imageio.mimsave(filename, frames, fps=30)\n",
    "\n",
    "\n",
    "def plot_metrics(scores, epsilons, filename=\"metrics.png\"):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(scores)\n",
    "    plt.title('Score per Episode vs Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epsilons)\n",
    "    plt.title('Epsilon vs Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=500*1000)\n",
    "        self.gamma = 0.95  # discount factor\n",
    "        self.epsilon = 1.0  # exploration-exploitation trade-off\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def save_weights(self, filepath):\n",
    "        self.model.save_weights(filepath)\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')  # adjust the environment as needed\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    episodes = 1000\n",
    "    batch_size = 32\n",
    "\n",
    "    # initialize metric arrays \n",
    "    scores, epsilons = [], []\n",
    "\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        total_reward = 0\n",
    "\n",
    "        for time in range(500):  # adjust the max number of steps as needed\n",
    "            action = agent.act(state)\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode: {}/{}, Total Reward: {}\".format(episode + 1, episodes, total_reward))\n",
    "                break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "        # track and plot metrics \n",
    "        scores.append(total_reward)\n",
    "        epsilons.append(agent.epsilon)\n",
    "        plot_metrics(scores, epsilons, filename=\"two-networks/cartpole_dqn_metrics.png\")\n",
    "\n",
    "        # update target network every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_model()\n",
    "            create_video(agent, env, filename=\"two-networks/cartpole_dqn_video_{}.mp4\".format(episode))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights every 50 episodes (adjust as needed)\n",
    "weights_file = f'two-networks/dqn_weights.h5'\n",
    "# agent.save_weights(weights_file)\n",
    "agent.model.save_weights(weights_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent.epsilon = 0.00\n",
    "create_video(agent, env, filename=\"two-networks/cartpole_dqn_video_final.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
